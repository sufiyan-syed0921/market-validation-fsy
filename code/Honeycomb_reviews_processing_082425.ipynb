{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# Documentation Block \n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#  ---------------- PROGRAM INFORMATION ---------------------------------------------------------------------------------\n",
    "# Name: Product review data processing\n",
    "# Author: Sufiyan Syed \n",
    "# Date: 09/22/2024\n",
    "\n",
    "#  ---------------- PURPOSE ---------------------------------------------------------------------------------------------\n",
    "# This program is designed to proccess product review data extracted from CSV files. It performs several key tasks:\n",
    "# 1. Counts and displays the number of rows in each CSV file, as well as the total number of rows across all files.\n",
    "# 2. Concatenates multiple CSV files into a single DataFrame.\n",
    "# 3. Cleans the data by removing duplicates and unnecessary columns.\n",
    "# 4. Translates non-English review titles and review texts to English using the Google Translate API.\n",
    "# 5. Saves the cleaned and consolidated data into an Excel file for further analysis.\n",
    "\n",
    "#  ---------------- SELECTS ---------------------------------------------------------------------------------------------\n",
    "# The program selects:\n",
    "# - CSV files that start with the prefix 'honeycomb' and end with '.csv' from the current working directory.\n",
    "# - Specific columns from the concatenated DataFrame, such as 'Reviewer' and 'Review Text', to remove duplicates.\n",
    "# - Review title and text for translation if the text is not already in English.\n",
    "\n",
    "#  ---------------- NOTES -----------------------------------------------------------------------------------------------\n",
    "# - The program assumes that review titles and texts may contain non-English characters and uses Google Translate to convert them to English.\n",
    "# - The Google Translate API is used with the 'auto' source language detection. Ensure internet connectivity when running this section.\n",
    "# - Make sure the CSV files have consistent column structures for concatenation to work correctly.\n",
    "# - The final cleaned DataFrame is saved as an Excel file named 'velocityone_data50524.xlsx'.\n",
    "# - The program includes basic checks to ensure duplicates are correctly removed and translations are performed where necessary.\n",
    "\n",
    "#  ---------------- DEPENDENCIES --------------------------------------------------------------------------------\n",
    "# Load Packages \n",
    "import pandas as pd  # pandas is used for reading CSV files into DataFrames, cleaning, manipulating the data, \n",
    "                     # and saving it to Excel or CSV files.\n",
    "\n",
    "import openpyxl  # openpyxl is used to work with Excel files, enabling the program to save the cleaned data as an Excel file.\n",
    "\n",
    "from googletrans import Translator  # Translator from googletrans is used to automatically detect and translate \n",
    "                                    # review text and titles to English from various languages.\n",
    "\n",
    "import numpy as np  # numpy is used for efficient array operations and can handle missing values \n",
    "                    # (e.g., using NaN for missing data) in the DataFrame.\n",
    "\n",
    "import string  # string provides utilities to work with strings, such as checking for punctuation, \n",
    "               # and manipulating text data (e.g., review content).\n",
    "\n",
    "import traceback  # traceback is used to print detailed error messages and stack traces, helping to debug the program \n",
    "                  # by identifying the exact point where an error occurred.\n",
    "\n",
    "import os  # os is used to interact with the operating system, such as getting the current working directory \n",
    "           # and listing files in a directory.\n",
    "\n",
    "from datetime import datetime # get current date \n",
    "\n",
    "#  ---------------- ENVIORNMENT ----------------------------------------------------------------------------------\n",
    "# Create program metadata \n",
    "filpath = os.getcwd()\n",
    "prgname = os.path.splitext(os.path.basename(filpath))[0]\n",
    "prgdate = datetime.now().strftime('%y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: honeycombxpc_tr_5s_p9.csv, Rows: 10\n",
      "File: honeycombxpc_tr_1s_p1.csv, Rows: 10\n",
      "File: honeycombxpc_other_1s_1.csv, Rows: 8\n",
      "File: honeycombxpc_tr_5s_p8.csv, Rows: 10\n",
      "File: honeycombxpc_tr_1s_p2.csv, Rows: 2\n",
      "File: honeycombxpc_tr_5s_p10.csv, Rows: 10\n",
      "File: honeycomb_otherreviews.csv, Rows: 32\n",
      "File: honeycombxpc_other_3s_1.csv, Rows: 8\n",
      "File: honeycombxpc_other2_3s_1.csv, Rows: 10\n",
      "File: honeycombxpc_tr_2s_p1.csv, Rows: 4\n",
      "File: honeycombxpc_other2_3s_2.csv, Rows: 6\n",
      "File: honeycombxpc_tr_3s_p1.csv, Rows: 10\n",
      "File: honeycombxpc_mr_5s_p10.csv, Rows: 10\n",
      "File: honeycombxpc_other2_1s_2.csv, Rows: 1\n",
      "File: honeycombxpc_tr_3s_p2.csv, Rows: 6\n",
      "File: honeycombxpc_mr_5s_p9.csv, Rows: 10\n",
      "File: honeycombxpc_other2_1s_1.csv, Rows: 10\n",
      "File: honeycombxpc_mr_5s_p8.csv, Rows: 10\n",
      "File: honeycombxpc_mr_5s_p5.csv, Rows: 10\n",
      "File: honeycombxpc_mr_5s_p4.csv, Rows: 10\n",
      "File: honeycombxpc_mr_5s_p6.csv, Rows: 10\n",
      "File: honeycombxpc_mr_5s_p7.csv, Rows: 10\n",
      "File: honeycombxpc_mr_5s_p3.csv, Rows: 10\n",
      "File: honeycombxpc_other_2s_1.csv, Rows: 2\n",
      "File: honeycombxpc_mr_5s_p2.csv, Rows: 10\n",
      "File: honeycombxpc_mr_5s_p1.csv, Rows: 10\n",
      "File: honeycombxpc_tr_5s_p1.csv, Rows: 10\n",
      "File: honeycombxpc_tr_5s_p3.csv, Rows: 10\n",
      "File: honeycombxpc_other2_2s_1.csv, Rows: 4\n",
      "File: honeycombxpc_tr_5s_p2.csv, Rows: 10\n",
      "File: honeycombxpc_tr_5s_p6.csv, Rows: 10\n",
      "File: honeycombxpc_tr_4s_p3.csv, Rows: 8\n",
      "File: honeycombxpc_tr_4s_p2.csv, Rows: 10\n",
      "File: honeycombxpc_tr_5s_p7.csv, Rows: 10\n",
      "File: honeycombxpc_tr_5s_p5.csv, Rows: 10\n",
      "File: honeycombxpc_tr_4s_p1.csv, Rows: 10\n",
      "File: honeycombxpc_tr_5s_p4.csv, Rows: 10\n",
      "\n",
      "Total number of rows across all files: 341\n"
     ]
    }
   ],
   "source": [
    "#  ---------------- DATA ---------------------------------------------------------------------------------------\n",
    "\n",
    "# Output directories \n",
    "savdir1 = os.getcwd()\n",
    "\n",
    "# Output files\n",
    "savfil1 = os.path.join(savdir1, \"productname_data_date.xlsx\")\n",
    "\n",
    "# Input directories & files\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# Inspect Row Counts for Product Review Data Files\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# Note that this program assumes csv files of review data are present in your current working directory. Alter the code\n",
    "# under \"Directory containing the CSV files\" to specify the directory you want the program to look for your files\n",
    "\n",
    "## Define Function to count rows\n",
    "def count_rows(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    return len(df)\n",
    "\n",
    "## Directory containing the CSV files\n",
    "csv_directory = os.getcwd()\n",
    "\n",
    "## List all CSV files in the directory\n",
    "csv_files = [os.path.join(csv_directory, file) for file in os.listdir(csv_directory) if file.startswith('honeycomb') and file.endswith('.csv')]\n",
    "\n",
    "## Dictionary to store counts for each file\n",
    "file_counts = {}\n",
    "\n",
    "## Variable to store the total number of rows\n",
    "total_rows = 0\n",
    "\n",
    "## Loop through CSV files and count rows\n",
    "for file in csv_files:\n",
    "    rows_count = count_rows(file)\n",
    "    file_name = os.path.basename(file)\n",
    "    file_counts[file_name] = rows_count\n",
    "    total_rows += rows_count\n",
    "    print(f\"File: {file_name}, Rows: {rows_count}\")\n",
    "\n",
    "## Display the total number of rows\n",
    "print(f\"\\nTotal number of rows across all files: {total_rows}\")\n",
    "\n",
    "## Clean enviornment\n",
    "del csv_directory, file_counts, total_rows, file, file_name, rows_count \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 341\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# Append Data Files\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "## Loop through CSV files and append them to the list of DataFrames\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file, index_col=False)\n",
    "    dfs.append(df)\n",
    "\n",
    "## Concatenate all DataFrames into a single DataFrame\n",
    "appended_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "## Count rows to compare to individual totals\n",
    "print(\"total rows:\", appended_df.shape[0])\n",
    "\n",
    "## Clean enviornment\n",
    "del df, dfs, file, csv_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colnames: ['Reviewer', 'Star Rating', 'Review Title', 'Review Date', 'Purchase Verification', 'Review Text', 'Reviewer ', 'Review Site']\n",
      "Filtered nrow 247\n",
      "Number of Duplicates: 94\n",
      "Check\n",
      "Number of Duplicates: 94.0\n",
      "Remaining Duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# Clean Data\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Remove index\n",
    "tmp = appended_df.drop(appended_df.columns[0], axis=1)\n",
    "#tmp2 = tmp.drop(['Unamed'])\n",
    "\n",
    "## Remove duplicates\n",
    "colnames = tmp.columns.tolist()\n",
    "print(\"Colnames:\", colnames)\n",
    "\n",
    "tmp2 = tmp.drop_duplicates(subset=['Reviewer', 'Review Text'], keep='first')\n",
    "print(\"Filtered nrow\", tmp2.shape[0])\n",
    "\n",
    "\n",
    "### Checks\n",
    "\n",
    "### Count duplicates\n",
    "print(\"Number of Duplicates:\", appended_df.duplicated(subset=['Reviewer', 'Review Text']).sum())\n",
    "\n",
    "### Add count of filtered dataframe to count of duplicates. \n",
    "### If sum equals total rows of original dataframe, print \"check\". \n",
    "if tmp2.shape[0] + appended_df.duplicated(subset=['Reviewer', 'Review Text']).sum() == appended_df.shape[0]: print(\"Check\")\n",
    "\n",
    "### Count duplicates\n",
    "tmp3 = tmp.duplicated(subset=['Reviewer', 'Review Text'], keep=False)\n",
    "tmp4 = tmp[tmp3]\n",
    "print(\"Number of Duplicates:\", tmp4.shape[0]/2)\n",
    "\n",
    "### Check if any duplicates are remaining\n",
    "print(\"Remaining Duplicates:\", tmp2.duplicated(subset=['Reviewer', 'Review Text']).sum())\n",
    "\n",
    "## Clean enviornment \n",
    "tmp = tmp2\n",
    "\n",
    "del tmp3, tmp4, tmp2, appended_df, colnames\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean review title\n",
    "tmp['review_title'] = tmp['Review Title'].str.split('out of 5 stars', n=1).str[-1]\n",
    "\n",
    "### Checks \n",
    "tmp2 = tmp[['review_title', 'Review Title']]\n",
    "\n",
    "### Delete old column\n",
    "tmp3 = tmp.drop(columns='Review Title')\n",
    "\n",
    "## Drop review site column\n",
    "tmp4 = tmp3.drop(columns='Review Site')\n",
    "\n",
    "## Drop duplicate \"Reviewer\" column\n",
    "tmp5 = tmp4.drop(tmp4.columns[5], axis=1)\n",
    "\n",
    "# Clean evniornment\n",
    "tmp = tmp5 \n",
    "\n",
    "del tmp2, tmp3, tmp4, tmp5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating: Bonjour tout le monde\n",
      "Translating to English: Hello everyone\n",
      "Hello everyone\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# Translate any non-english reviews to english\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Define translation function\n",
    "def translate_to_english(text, src_language='auto'):\n",
    "    try:\n",
    "        if pd.isnull(text):\n",
    "            return text\n",
    "        elif all(char in string.punctuation or char.isspace() for char in text):\n",
    "            print(\"Skipping punctuation-only text:\", text)\n",
    "            return text\n",
    "        else: \n",
    "            translator = Translator()\n",
    "            print(\"Translating:\", text)\n",
    "            translated_text = translator.translate(text, src=src_language, dest='en')\n",
    "            print(\"Translating to English:\", translated_text.text)\n",
    "            return translated_text.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during translation: {e}\")\n",
    "        print(f\"Original text: {text}\")\n",
    "        traceback.print_exc()  # Print the traceback for debugging\n",
    "        return text\n",
    "\n",
    "## Test function\n",
    "text_to_translate = \"Bonjour tout le monde\"  # French text\n",
    "translated_text = translate_to_english(text_to_translate)\n",
    "print(translated_text)  # Output: Hello everyone\n",
    "\n",
    "del text_to_translate, translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Translate review title and text\n",
    "tmp2 = tmp\n",
    "\n",
    "## Review titles\n",
    "tmp2['review_title_oeng'] = tmp.apply(lambda row: translate_to_english(row['review_title']) \n",
    "                                 if row['review_title'] != 'en' \n",
    "                                 else row['review_title'], axis=1)\n",
    "\n",
    "## Review text\n",
    "tmp2['review_text_oeng'] = tmp.apply(lambda row: translate_to_english(row['Review Text']) \n",
    "                                 if row['Review Text'] != 'en'\n",
    "                                 else row['Review Text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checks\n",
    "\n",
    "### Spot check\n",
    "tmp3 = tmp2[['Review Text', 'review_title', 'review_text_oeng', 'review_title_oeng']]\n",
    "\n",
    "### View translated items\n",
    "tmp4 = tmp3[(tmp3['Review Text'] != tmp3['review_text_oeng']) & (tmp3['review_title'] != tmp3['review_title_oeng'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# Clean and save\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "del tmp3, tmp4, tmp \n",
    "\n",
    "## Clean names\n",
    "tmp2.columns = tmp2.columns.str.strip().str.lower().str.replace(' ', '_' )\n",
    "\n",
    "## Save the appended DataFrame to an Excel file\n",
    "tmp2.to_excel(savfil1, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minima_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
