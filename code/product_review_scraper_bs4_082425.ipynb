{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# Documentation \n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#  ---------------- PROGRAM INFORMATION ---------------------------------------------------------------------------------\n",
    "# Name: Product Review Webscraping Program\n",
    "# Author: Sufiyan Syed\n",
    "# Date: 09/22/2024\n",
    "\n",
    "#  ---------------- PURPOSE ---------------------------------------------------------------------------------------------\n",
    "# This program is designed to scrape product reviews from an Amazon product page using a specified URL input.\n",
    "# It extracts details such as the reviewer name, star rating, review title, review date, purchase verification,\n",
    "# and the review text. The extracted reviews are stored in a list and saved as a CSV file for further analysis.\n",
    "\n",
    "#  ---------------- SELECTS ---------------------------------------------------------------------------------------------\n",
    "# This program uses the url of the product reviews webpage as the input for this program. \n",
    "\n",
    "#  ---------------- NOTES -----------------------------------------------------------------------------------------------\n",
    "# The program only extracts the reviews present on the url's webpage. Pagination of the reviews webpage must be \n",
    "# done manually with seperate URL inputs. \n",
    "\n",
    "#  ---------------- DEPENDENCIES --------------------------------------------------------------------------------\n",
    "# Load Packages \n",
    "from urllib.request import Request, urlopen # Request and urlopen are used to send a request to the URL and open the \n",
    "                                            # webpage for scraping. \n",
    "\n",
    "from bs4 import BeautifulSoup # BeautifulSoup is used to parse the HTML content of the webpage and extract specific \n",
    "                              # data elements (e.g., reviews, ratings, titles).\n",
    "\n",
    "import pandas as pd # pandas is used to create and manipulate a DataFrame, which holds the extracted review data,\n",
    "                    # and to save it as a CSV file.\n",
    "\n",
    "import os # get current working directory \n",
    "\n",
    "from datetime import datetime # get current date \n",
    "\n",
    "#  ---------------- DATA ---------------------------------------------------------------------------------------\n",
    "# Output directories \n",
    "savdir1 = os.getcwd()\n",
    "\n",
    "# Output files\n",
    "savfil1 = os.path.join(savdir1, \"productname_category_star_#.csv\")\n",
    "\n",
    "#  ---------------- ENVIORNMENT ----------------------------------------------------------------------------------\n",
    "# Create program metadata \n",
    "filpath = os.getcwd()\n",
    "prgname = os.path.splitext(os.path.basename(filpath))[0]\n",
    "prgdate = datetime.now().strftime('%y%m%d')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# Prepare function to extract reviews\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Add headers\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\"}\n",
    "\n",
    "# Define function to get webpage results into Beautifulsoup object \n",
    "def get_page(url, headers):\n",
    "    try:\n",
    "        encoded_headers = {key: value.encode('utf-8') for key, value in headers.items()}\n",
    "        req = Request(url, headers=encoded_headers)\n",
    "        page = urlopen(req)\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        return soup\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# Gather HTML from URL\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# Replace the url in get_page() for each page of reviews. \n",
    "\n",
    "# Loop until header1 is not None\n",
    "while True:\n",
    "    # Get html\n",
    "    soup = get_page(\"https://www.amazon.com/Thrustmaster-Yoke-PACK-Boeing-Xbox-x/dp/B09DPYHM55/ref=sr_1_5?crid=29KZ42OY0C859&dib=eyJ2IjoiMSJ9.gQXGQTXbcMi54OeLHcBcCdy8bUzAiP_khKF2HF5sDWteRLmjwsCoEm-T8ES3RJDk_pwvrXj5zel4nKbkyOA8Kc_d9eOe7P4LdnBf0ENU7pdjnW7xTK1Qdyl53s_jPhNO8xJh635sqWj92SiLQhRkqB6v3zgWbPwxbaXYM6A05uYAoi3s7d1u3lfAfrqrdOXxUbA2DWaManb3dv3M-e02zwsWrCouonZwu86t9YX1RCs.cKc6vAHkdCKax4iiYjtUPIMLhnIJikbZP9z_UlLYEaM&dib_tag=se&keywords=thrustmaster&qid=1733025366&sprefix=thrustmas%2Caps%2C203&sr=8-5&th=1\", headers=headers)\n",
    "\n",
    "    # Get h1\n",
    "    header1 = soup.find('h1')\n",
    "\n",
    "    # Check if header1 is not None\n",
    "    if header1 is not None:\n",
    "        break  # Exit the loop if header1 is not None\n",
    "    \n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# Run Checks If Needed \n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#print(soup)\n",
    "#print(\"--------------\")\n",
    "#print(header1)\n",
    "#print(header1.attrs)\n",
    "#product_title = header1.get_text().strip()\n",
    "#print(product_title)\n",
    "#all_headers = soup.find_all('h1')\n",
    "#print(all_headers)\n",
    "#type(all_headers)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# Extract Review Data \n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create an empty list to store dictionaries for each review\n",
    "reviews_list = []\n",
    "\n",
    "review_content = soup.find_all('div', {'data-hook': 'review'})\n",
    "\n",
    "# Extract contents\n",
    "for review in review_content:\n",
    "    reviewer_name = review.find('span', {'class': 'a-profile-name'}).get_text().strip()\n",
    "    star_rating_tag = review.find('i', {'data-hook': ['review-star-rating', 'cmps-review-star-rating']})\n",
    "    star_rating = star_rating_tag.get_text().strip() if star_rating_tag else \"No Rating\"\n",
    "    review_title_tag = review.find('a', {'data-hook': 'review-title'})\n",
    "    # If the review title is not found within an <a> tag, try finding it within a <span> tag\n",
    "    if review_title_tag is None:\n",
    "        review_title_tag = review.find('span', {'data-hook': 'review-title'})\n",
    "    # Extract the text of the review title from the tag, if found\n",
    "    if review_title_tag is not None:\n",
    "        review_title = review_title_tag.get_text().strip()\n",
    "    else:\n",
    "        review_title = \"Title not found\"\n",
    "    review_date = review.find('span', {'data-hook': 'review-date'}).get_text().strip()\n",
    "    purchase_verification_tag = review.find('span', {'data-hook': 'avp-badge'})\n",
    "    purchase_verification = purchase_verification_tag.get_text().strip() if purchase_verification_tag else \"\"\n",
    "    review_text_spans = review.find_all('span', {'data-hook': 'review-body'})\n",
    "    review_text = ' '.join([span.get_text().strip() for span in review_text_spans])\n",
    "\n",
    "    # Append review as a dictionary to the list\n",
    "    reviews_list.append({'Reviewer': reviewer_name,\n",
    "                         'Star Rating': star_rating,\n",
    "                         'Review Title': review_title,\n",
    "                         'Review Date': review_date,\n",
    "                         'Purchase Verification': purchase_verification,\n",
    "                         'Review Text': review_text})\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# Output\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(reviews_list)\n",
    "\n",
    "# Save\n",
    "df.to_csv(savfil1, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
